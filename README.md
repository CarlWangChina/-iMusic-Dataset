# -iMusic-Dataset
1 √†iMusic Dataset To enrich the music styles of our dataset, we built the √†iMusic Dataset1 from scratch. We first collected 6,000 guitar scores, where we selected 650 oriental songs that meet our requirements. Fur- thermore, several music professionals are invited to manually stan- dardize the naming rules and formats of our experimental data. We also split each song into mutually-different sections to reduce data repetition. Segments with different tonalities in the same song will also be listed separately. After 4 months of collection and process- ing, √†iMusic Dataset is finally completed, containing 2323 musical pieces. Our √†iMusic Dataset is mainly based on the Chinese national pen- tatonic, which is composed of five positive tones, namely, "Gong(Do), Shang(Re), Jue(Mi), Zhi(Sol) and Yu(La)" and various partial tones. To better train the SongDriver on our original √†iMusic Dataset, we transpose the na- tional pentatonic into a natural major with Gong as the tonic. 2 Data Processing These datasets are further standardized following the steps below. 2.1 Rhythm Screening. We only reserve the music pieces in 4/4 and 2/4 time for subsequent sampling. To maintain a stable sampling granularity, pieces containing chords shorter than one beat are also deleted. 2.2 Octave Transposition. For each piece of music, we calculate the current octave of the melody and accompaniment according to the note distribution. Then by adding to or subtracting several interval differences, we transpose the melody to the 6ùë°h row of the MIDI standard pitch table and the accompaniment to the 4ùë°h row. 2.3 Mode Unification. Based on the music mode information in the dataset, we convert all major mode music to C major and all minor mode music to A minor. The distinguishment between major mode and minor mode is important because the extraction of our proposed four features is influenced by the mode of current music.
